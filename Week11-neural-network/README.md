## Reference
### Tool1
https://phiresky.github.io/neural-network-demo/
### Tool2
https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.64443&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false
### Tool3
https://www.deeplearning.ai/ai-notes/initialization/
___
### Softmax as the activation function in the last layers of multi-class problems
https://www.analyticsvidhya.com/blog/2021/04/introduction-to-softmax-for-neural-network/

## Should read [P.413 ~ 459]
https://github.com/probml/pml-book/releases/latest/download/book1.pdf </br>
https://www.3blue1brown.com/lessons/backpropagation-calculus </br>
https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ </br>
https://stats.stackexchange.com/a/561467 </br>
why not need to compute delta (error) of bias unit https://stats.stackexchange.com/a/130605

```
In the forward propagate stage, the data flows through the network to get the outputs. 
The loss function is used to calculate the total error (sum of squared error). Then, we use backward propagation algorithm to 
calculate the gradient of the loss function with respect to each weight and bias.
```
## Great resources
1. <b>Backpropagation:</b> https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ 
2. <b>Backpropagation:</b> Backpropagation: http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/
3. Interesting paper: Neural circuit policies enabling auditable autonomy 
&nbsp;&nbsp;&nbsp; [![Neural circuit policies](https://img.youtube.com/vi/wAa358pNDkQ/0.jpg)](https://www.youtube.com/watch?v=wAa358pNDkQ)
